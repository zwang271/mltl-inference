\documentclass[12pt]{article}
% \documentclass{elsarticle} %A different option for format styling...

%NOTE: Use of this template is *totally optional* -- it is just provided to make your life easier. If it does not do that, feel free to use your own template. The formatting does not count for points, only the answers in a readable (LaTeX-generated) format count. 

\textwidth 6.5in
\oddsidemargin 0.0in %this is a 1-inch margin
\evensidemargin 1.0in %matching 1-inch margin

\usepackage{amssymb}
\usepackage{alltt}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{mathrsfs} %for \mathscr{} 
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{gensymb} %for \degree
\usepackage{longtable} %for longtabu
\usepackage{hhline} %for double \hline in longtabu
\usepackage{blindtext}

\newtheorem{defin}{Definition}
\newtheorem{intuit}{Intuition}

%Here are the commands included in elsarticle style:
\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}

\interfootnotelinepenalty=10000

\renewcommand{\phi}{\varphi}
\newcommand{\always}{\Box}
\newcommand{\eventually}{\Diamond}
\newcommand{\calL}{{\cal L}}

\newcommand{\pspic}[2]{\scalebox{#1}{\includegraphics{#2}}}



%Number the Exercises with one counter through multiple sections
\newcounter{ExerciseCounter}
\setcounter{ExerciseCounter}{1} %start counting at 1


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figure Magic
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{epsfig}
\usepackage{float}
\usepackage{subfigure}
\usepackage{wrapfig}
\renewcommand{\topfraction}{.95} %figures can take up at most 95% of the page before being alone
\renewcommand{\bottomfraction}{.99} %figures can take up at most 99% of the page before being alone
\renewcommand{\textfraction}{.1} %at most this this % of page will be text before making figure-only page
\addtolength{\abovecaptionskip}{-3mm}


\begin{document}

\title{\bf
\large COM S 572 : Principles of Artificial Intelligence \\
\large MLTL Inference -- Summary}

\author{
  Luke Marzen\\
  \texttt{ljmarzen@iastate.edu}
  \and
  Jayaraman, Swaminathan\\
  \texttt{swamjay@iastate.edu}
  \and
  Nhan Tran\\
  \texttt{nhtran@iastate.edu}
  \and
  Zili Wang\\
  \texttt{ziliw1@iastate.edu}
}

\date{March 29, 2024}

\maketitle

% INSTRUCTIONS FROM CANVAS
% Each group submit via Canvas a summary that contains the project title, a brief summary of the goals of the project, the basic approach to be followed, and anticipated results along with the relevant bibliography

We provide an update on our project, MLTL Inference, and give a summary of our work so far.
To recap, we'll restate the problem statement:
Given a set of traces $T = T^+ \cup T^-$ over $n$ variables, the goal is to learn a MLTL formula $\phi$ such that for all $\pi \in T^+$, $\pi \models \phi$, and for all $\pi \in T^-$, $\pi \not\models \phi$.
The traces $T^+$ represent the positive examples, or desirable behaviors, and $T^-$ represent the negative examples, or undesirable behaviors.
The goal is to learn a short and generalizable MLTL formula that separates the positive and negative examples.

Previously, Zili Wang has developed and evaluated Genetic Programming (GP) based approach to learning MLTL formulas, and lays the groundwork for the proposed project.
The repository containing the code and datasets for this project can be found at 

\noindent \url{https://github.com/zwang271/mltl-inference}.
Work that will be reused includes the following components:
\begin{enumerate}
  \item A parser (Python) for MLTL formulas, that can compute various properties of the formula, such as the number of atomic propositions, the syntax tree, worst propagation delay (wpd), and various other useful functions.  
  \item An MLTL interpreter (C++): on input trace $\pi$ and a MLTL formula $\phi$, determines if $\pi \models \phi$.
  \item A dataset generator (Python) that given an MLTL formula, uses either random trace sampling or regular expression sampling to generate a set of positive and negative examples.
  \item 9 datasets, each with 500 positive and 500 negative examples, split into 80\% training and 20\% testing sets.
\end{enumerate}

We provide a summary of the basic approaches to be followed:
\begin{itemize}
    \item Zili
    \item Luke - Informed search (A*)

This approach will formulate MLTL inference into a search problem then apply A*.  Multiple heuristic functions will be evaluated and compared.  Heuristic functions may consider factors such as accuracy against positive and negative traces and length or complexity of the formula.  Due to the search space growing exponentially as formula length increases it will likely be necessary to employ inadmissible heuristics which could greatly accelerate the search but could result in suboptimal solutions.  An analysis will be conducted to assess the trade-offs between execution time and solution optimality using admissible versus inadmissible heuristic functions. 

The formulating this into a search problem is non-trivial. Here is an overview of the formulation. The initial state will be an empty formula. Each out-going edge will either add a new propositional or temporal operator to the formula or revise the bounds of a temporal operator.  When temporal operators are added the must be given initial bounds. There are multiple approaches that can be taken to determine the bounds. One method could be generating every possible set of bounds up to the future reach of the longest expected trace. An alternative approach would be to initially generate bounds that cover the entire future reach of the longest expected traces, then have edges which effectively allow for revising of the bounds, say by dividing the bounds in half or stepping on of the bounds by some amount. The alternative approach as potential advantage of encouraging bounds that may have better general representation of the target, which may help guide the search of the enormous state space.
    \item Swaminathan
    \item Nhan

Preparing data is necessary before training models. Currently, the formulas are outputted as text that needs to be tokenized along with the traces. Traces also require a symbol to represent their validity: positive or negative. Custom tokenization for the traces and the formulas has been implemented. In terms of the models, the current approach involves sequence-to-sequence translation with an LSTM encoder and transformers as the decoder. However, there are risks of losing information since transformers are limited by input size. Recent work has been done on a recurrent memory transformer that could also be helpful in tackling this problem. The expected outcome is a model that can produce a reasonable accuracy in determining formulas that can be further refine by other algorithms.
\end{itemize} 


After implementation of each approach we will compare the inference time and formula accuracy on input data sets of ranging length and complexity.


\end{document}
